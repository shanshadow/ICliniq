{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48907,
     "status": "ok",
     "timestamp": 1727933247640,
     "user": {
      "displayName": "Sujit S",
      "userId": "14848334329831765264"
     },
     "user_tz": -330
    },
    "id": "hgcVyfm50-i6",
    "outputId": "9ef211ed-5edf-4b6f-f2c8-337a5f65eb54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sudha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\sudha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sudha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 11.2 MB/s eta 0:00:00\n",
      "Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17519 sha256=7928d7bc118ae6ff08b64eb7f305ea63994ca2696fb4c9fe31f2517935ee3be8\n",
      "  Stored in directory: c:\\users\\sudha\\appdata\\local\\pip\\cache\\wheels\\95\\0f\\04\\b17a72024b56a60e499ce1a6313d283ed5ba332407155bee03\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: hyperframe\n",
      "    Found existing installation: hyperframe 6.1.0\n",
      "    Uninstalling hyperframe-6.1.0:\n",
      "      Successfully uninstalled hyperframe-6.1.0\n",
      "  Attempting uninstall: hpack\n",
      "    Found existing installation: hpack 4.1.0\n",
      "    Uninstalling hpack-4.1.0:\n",
      "      Successfully uninstalled hpack-4.1.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 5.2.0\n",
      "    Uninstalling chardet-5.2.0:\n",
      "      Successfully uninstalled chardet-5.2.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: h2\n",
      "    Found existing installation: h2 4.2.0\n",
      "    Uninstalling h2-4.2.0:\n",
      "      Successfully uninstalled h2-4.2.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.7\n",
      "    Uninstalling httpcore-1.0.7:\n",
      "      Successfully uninstalled httpcore-1.0.7\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: googletrans\n",
      "    Found existing installation: googletrans 4.0.2\n",
      "    Uninstalling googletrans-4.0.2:\n",
      "      Successfully uninstalled googletrans-4.0.2\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MBartForCausalLM' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/mbart-large-50\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     10\u001b[0m translator \u001b[38;5;241m=\u001b[39m Translator()\n",
      "File \u001b[1;32mc:\\Users\\sudha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MBartForCausalLM' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "!pip install googletrans==4.0.0-rc1\n",
    "from googletrans import Translator\n",
    "\n",
    "# Load a pre-trained conversational model (mBART is multilingual)\n",
    "model_name = \"facebook/mbart-large-50\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "translator = Translator()\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_response(user_input, target_language):\n",
    "    # Translate input to English for the model\n",
    "    translated_input = translator.translate(user_input, dest='en').text\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer.encode(translated_input, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Translate the model's response back to the target language\n",
    "    translated_response = translator.translate(response, dest=target_language).text\n",
    "    return translated_response\n",
    "\n",
    "# Main conversation loop\n",
    "def chat():\n",
    "    print(\"Welcome to the multi-lingual translator bot! (Type 'quit' to exit)\")\n",
    "    languages = {\n",
    "    'English': 'en',\n",
    "    'Spanish': 'es',\n",
    "    'French': 'fr',\n",
    "    'German': 'de',\n",
    "    'Chinese': 'zh-cn',\n",
    "    'Hindi': 'hi',\n",
    "    'Arabic': 'ar'\n",
    "    }\n",
    "    print('Available languages:')\n",
    "    for lang, code in languages.items():\n",
    "        print(f'{code} : {lang}')\n",
    "    language = input(\"Select your language: \").lower()\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Generate bot response based on user input and preferred language\n",
    "        bot_response = generate_response(user_input, language)\n",
    "        print(f\"Bot: {bot_response}\")\n",
    "\n",
    "# Start the conversation\n",
    "if __name__ == '__main__':\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QuSg16X160c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPYTufok1j632Qy0oxw1rTg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
